<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>多模态（语言、视觉）</title>
    <meta name="description" content="">
    <meta name="generator" content="VuePress 1.3.0">
    
    
    <link rel="preload" href="/github-nlp-progress/assets/css/0.styles.6033efd5.css" as="style"><link rel="preload" href="/github-nlp-progress/assets/js/app.03014354.js" as="script"><link rel="preload" href="/github-nlp-progress/assets/js/2.3456af6e.js" as="script"><link rel="preload" href="/github-nlp-progress/assets/js/3.937c9f03.js" as="script"><link rel="preload" href="/github-nlp-progress/assets/js/54.9f32cbb0.js" as="script"><link rel="prefetch" href="/github-nlp-progress/assets/js/10.cb15689e.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/11.eabde15f.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/12.27303af9.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/13.1ecc9616.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/14.f7e0a6d6.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/15.c908a65f.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/16.77a77821.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/17.d273c85c.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/18.1090edb4.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/19.c36f96a3.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/20.c42e5144.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/21.179a6c72.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/22.49515d36.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/23.e1aedaab.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/24.4385c2c6.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/25.84428cdb.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/26.120fe404.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/27.e84466dc.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/28.ecce4807.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/29.dd2817fc.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/30.28a3fb74.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/31.19988088.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/32.9a5cc67c.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/33.54b6fa31.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/34.c0e98bd0.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/35.2c79ac06.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/36.bff0078c.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/37.927d231a.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/38.005d0d92.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/39.3cadeae4.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/4.d939385a.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/40.58ef4623.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/41.b9c65150.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/42.af42e747.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/43.4dcb7194.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/44.dacd7b7a.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/45.54a535b7.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/46.0f0eff8a.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/47.5694acf3.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/48.946f37ab.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/49.34dda679.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/5.ffcec782.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/50.bba2f765.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/51.7cfe7fbe.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/52.1369c4c1.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/53.493d9eb5.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/55.cfc8e268.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/56.1ae5b816.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/57.15a35255.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/58.0cbba9ae.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/59.cc060eb2.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/6.075143cf.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/60.f667b9ec.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/61.4a173092.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/62.c3282d48.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/63.5a8e3369.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/64.fbab04ca.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/65.015dee23.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/7.51360b5e.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/8.1f2d5736.js"><link rel="prefetch" href="/github-nlp-progress/assets/js/9.73d267d0.js">
    <link rel="stylesheet" href="/github-nlp-progress/assets/css/0.styles.6033efd5.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div id="global-layout" data-v-2be041bb><header class="bk-dark" data-v-2be041bb><video autoplay="autoplay" loop="loop" muted="muted" data-v-2be041bb><source src="/github-nlp-progress/assets/media/bk.417d52db.mp4" type="video/mp4" data-v-2be041bb></video> <div data-v-2be041bb><div class="header-content" data-v-2be041bb><h1 data-v-2be041bb>NLP-PROGRESS</h1> <h2 data-v-2be041bb>Repository to trasck the progress in Natural Language Processing (NLP), including the datasets and the current state-of-the-art for the most common NLP tasks.</h2> <a href="#" class="btn" data-v-2be041bb><i class="iconfont icon-github" data-v-2be041bb></i>
                    View on GitHub
                </a></div></div></header> <div class="theme-container no-navbar" data-v-2be041bb><!----> <div class="sidebar-mask"></div> <div class="sidebar-wrap sidebar-wrap-absolute"><aside class="sidebar"><!---->  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>目录</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/github-nlp-progress/multimodal/multimodal.html#多模态（语言、视觉）" class="sidebar-link">多模态（语言、视觉）</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/github-nlp-progress/multimodal/multimodal.html#视频分类" class="sidebar-link">视频分类</a></li><li class="sidebar-sub-header"><a href="/github-nlp-progress/multimodal/multimodal.html#视频内容识别" class="sidebar-link">视频内容识别</a></li><li class="sidebar-sub-header"><a href="/github-nlp-progress/multimodal/multimodal.html#视频人物识别" class="sidebar-link">视频人物识别</a></li><li class="sidebar-sub-header"><a href="/github-nlp-progress/multimodal/multimodal.html#视频增强和超分" class="sidebar-link">视频增强和超分</a></li><li class="sidebar-sub-header"><a href="/github-nlp-progress/multimodal/multimodal.html#多模内容生成" class="sidebar-link">多模内容生成</a></li></ul></li></ul></section></li></ul> </aside></div> <main class="page"> <div class="theme-default-content content__default"><h2 id="多模态（语言、视觉）"><a href="#多模态（语言、视觉）" class="header-anchor">#</a> 多模态（语言、视觉）</h2> <h3 id="视频分类"><a href="#视频分类" class="header-anchor">#</a> 视频分类</h3> <h4 id="_1-mtsvprc-dataset"><a href="#_1-mtsvprc-dataset" class="header-anchor">#</a> 1. MTSVPRC Dataset</h4> <ul><li><p><strong>数据集简介：</strong></p> <p>该数据集来自美图公司联合中国模式识别与计算机视觉学术会议共同举办的PRCV2018“美图短视频实时分类挑战赛”。数据集一共有10万，以短视频为主，长度约为5-15s。包含50个分类，舞蹈、唱歌、手工、健身等热门短视频类型，除了包含与人相关的一些行为类别，还有一些风景，宠物等类别。</p></li> <li><p><strong>数据集详情：</strong></p> <table><thead><tr><th style="text-align:center;">名称</th> <th style="text-align:center;">规模</th> <th style="text-align:center;">创建日期</th> <th style="text-align:center;">作者</th> <th style="text-align:center;">单位</th> <th style="text-align:center;">论文</th> <th style="text-align:center;">下载</th> <th style="text-align:center;">评测</th></tr></thead> <tbody><tr><td style="text-align:center;">MTSVRC数据集</td> <td style="text-align:center;">10万视频数据</td> <td style="text-align:center;">2018年</td> <td style="text-align:center;">N/A</td> <td style="text-align:center;">美图</td> <td style="text-align:center;">N/A</td> <td style="text-align:center;"><a href="https://github.com/MTCloudVision/MTSVRC" target="_blank" rel="noopener noreferrer">链接<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td style="text-align:center;"><a href="https://challenge.ai.meitu.com/mtsvrc2018/introduction.html" target="_blank" rel="noopener noreferrer">美图短视频实时分类挑战赛<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr></tbody></table></li> <li><p><strong>基于该数据集发表的相关方案：</strong></p> <ul><li>第一名：https://chuangke.aliyun.com/info/detail/673631?scm=20140722.184.2.173</li> <li>baseline：https://github.com/MTCloudVision/MTSVRC</li></ul></li></ul> <p><a href="/github-nlp-progress/" class="router-link-active">回到首页</a></p> <h3 id="视频内容识别"><a href="#视频内容识别" class="header-anchor">#</a> 视频内容识别</h3> <h4 id="_1-videonet-2019"><a href="#_1-videonet-2019" class="header-anchor">#</a> 1. VideoNet 2019</h4> <ul><li><p><strong>数据集简介：</strong></p> <p>该数据集来自视频内容识别挑战赛。为推动物体、场景多维度视频内容识别的应用，极链科技联合复旦大学推出多维度标注视频数据集VideoNet。包含300类视频，200类场景和200类物体的9万段视频，总时长4000小时。对视频进行事件分类标注，并针对每个镜头的关键帧进行了场景和物体两个维度的标注，充分体现了多维度内容之间的语义联系。</p></li> <li><p><strong>数据集详情：</strong></p> <table><thead><tr><th style="text-align:center;">名称</th> <th style="text-align:center;">规模</th> <th style="text-align:center;">创建日期</th> <th style="text-align:center;">作者</th> <th style="text-align:center;">单位</th> <th style="text-align:center;">论文</th> <th style="text-align:center;">下载</th> <th style="text-align:center;">评测</th></tr></thead> <tbody><tr><td style="text-align:center;">VideoNet</td> <td style="text-align:center;">近9万视频</td> <td style="text-align:center;">2019年</td> <td style="text-align:center;">N/A</td> <td style="text-align:center;">极链科技&amp;复旦大学</td> <td style="text-align:center;">N/A</td> <td style="text-align:center;"><a href="http://challenge.ai.videojj.com/#/lookback2019summer/ds" target="_blank" rel="noopener noreferrer">链接<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td style="text-align:center;"><a href="http://challenge.ai.videojj.com/#/lookback2019summer/intro" target="_blank" rel="noopener noreferrer">视频内容识别挑战赛<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr></tbody></table></li> <li><p><strong>基于该数据集发表的相关方案：</strong></p> <ul><li>baseline：https://github.com/xxbbml/VideoNet_Baseline</li> <li>第一名：http://www.ia.cas.cn/xwzx/kydt/201909/t20190905_5377262.html</li></ul></li></ul> <p><a href="/github-nlp-progress/" class="router-link-active">回到首页</a></p> <h3 id="视频人物识别"><a href="#视频人物识别" class="header-anchor">#</a> 视频人物识别</h3> <h4 id="_1-iqiyi-vid-dataset"><a href="#_1-iqiyi-vid-dataset" class="header-anchor">#</a> 1. IQIYI-VID-Dataset</h4> <ul><li><p><strong>数据集简介：</strong></p> <p>该数据集来自爱奇艺多模态视频人物识别挑战赛。爱奇艺联合中国模式识别与计算机视觉大会（PRCV2018）举办“多模态视频人物识别挑战赛”。该数据集为明星视频数据集，包含5000位明星艺人，总时长1000小时、50万条视频片段。</p></li> <li><p><strong>数据集详情：</strong></p> <table><thead><tr><th style="text-align:center;">名称</th> <th style="text-align:center;">规模</th> <th style="text-align:center;">创建日期</th> <th style="text-align:center;">作者</th> <th style="text-align:center;">单位</th> <th style="text-align:center;">论文</th> <th style="text-align:center;">下载</th> <th style="text-align:center;">评测</th></tr></thead> <tbody><tr><td style="text-align:center;">IQIYI-VID-Dataset</td> <td style="text-align:center;">50万条视频，5000名人物</td> <td style="text-align:center;">2018年</td> <td style="text-align:center;">N/A</td> <td style="text-align:center;">爱奇艺</td> <td style="text-align:center;"><a href="https://arxiv.org/pdf/1811.07548.pdf" target="_blank" rel="noopener noreferrer">链接<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td style="text-align:center;"><a href="http://challenge.ai.iqiyi.com/detail?raceId=5afc36639689443e8f815f9e" target="_blank" rel="noopener noreferrer">链接<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td style="text-align:center;"><a href="http://challenge.ai.iqiyi.com/detail?raceId=5afc36639689443e8f815f9e" target="_blank" rel="noopener noreferrer">爱奇艺多模态视频人物识别挑战赛<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr></tbody></table></li> <li><p><strong>基于该数据集发表的相关方案&amp;论文：</strong></p> <ul><li>第三名方案：https://github.com/LegenDong/IQIYI_VID_FACE_2019</li> <li>Jiankang D., Jia Guo. 2019. ArcFace: Additive Angular Margin Loss for Deep Face Recognition. CVPR. http://openaccess.thecvf.com/content_CVPR_2019/papers/Deng_ArcFace_Additive_Angular_Margin_Loss_for_Deep_Face_Recognition_CVPR_2019_paper.pdf</li> <li>Jianrong C., Li Yang., et al. 2019. A Novel Deep Multi-Modal Feature Fusion Method for Celebrity Video Identification. MM '19: Proceedings of the 27th ACM International Conference on Multimedia. https://dl.acm.org/doi/abs/10.1145/3343031.3356067</li> <li>Xi Fang., Ying Zou., 2019. Make the Best of Face Clues in iQIYI Celebrity VideoIdentification Challenge 2019. MM '19: Proceedings of the 27th ACM International Conference on Multimedia. https://dl.acm.org/doi/abs/10.1145/3343031.3356056</li></ul></li></ul> <p><a href="/github-nlp-progress/" class="router-link-active">回到首页</a></p> <h3 id="视频增强和超分"><a href="#视频增强和超分" class="header-anchor">#</a> 视频增强和超分</h3> <h4 id="_1-youku-vesr-dataset"><a href="#_1-youku-vesr-dataset" class="header-anchor">#</a> 1. Youku-VESR Dataset</h4> <ul><li><p><strong>数据集简介：</strong></p> <p>该数据集来自阿里巴巴优酷视频增强和超分辨率挑战赛。数据的每个样本由低分辨率和高分辨率的视频对组成。低分辨率视频为输入，高分辨率视频为增强和超分后的真值。每个视频的时间长度为5秒左右。大部分高清数据的分辨率是1080P，大概300M。由于是4倍超分，低质视频分辨率为270P，大概19M。 少量高清数据的分辨率是2048×1152，低质视频分辨率为512×288。视频数据为无压缩的y4m格式。比赛公开1000个视频，总量超过300GB。</p></li> <li><p><strong>数据集详情：</strong></p> <table><thead><tr><th style="text-align:center;">名称</th> <th style="text-align:center;">规模</th> <th style="text-align:center;">创建日期</th> <th style="text-align:center;">作者</th> <th style="text-align:center;">单位</th> <th style="text-align:center;">论文</th> <th style="text-align:center;">下载</th> <th style="text-align:center;">评测</th></tr></thead> <tbody><tr><td style="text-align:center;">优酷视频增强和超分数据集</td> <td style="text-align:center;">1000视频数据</td> <td style="text-align:center;">2019年</td> <td style="text-align:center;">N/A</td> <td style="text-align:center;">优酷</td> <td style="text-align:center;">N/A</td> <td style="text-align:center;"><a href="https://tianchi.aliyun.com/dataset/dataDetail?spm=5176.12281978.0.0.4bd14adaHjku69&amp;dataId=39568" target="_blank" rel="noopener noreferrer">链接<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td style="text-align:center;"><a href="https://tianchi.aliyun.com/competition/entrance/231711/introduction" target="_blank" rel="noopener noreferrer">阿里巴巴优酷视频增强和超分辨率挑战赛<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td></tr></tbody></table></li> <li><p><strong>基于该数据集发表的相关方案&amp;论文：</strong></p> <ul><li>Jiale Chen., Xu Tan., et al. 2020. VESR-Net: The Winning Solution to Youku Video Enhancement and Super-Resolution Challenge.</li></ul></li></ul> <p><a href="/github-nlp-progress/" class="router-link-active">回到首页</a></p> <h3 id="多模内容生成"><a href="#多模内容生成" class="header-anchor">#</a> 多模内容生成</h3> <h4 id="_1-flickr8k-cn、flickr30k-cn、coco-cn"><a href="#_1-flickr8k-cn、flickr30k-cn、coco-cn" class="header-anchor">#</a> 1. Flickr8K-CN、Flickr30K-CN、COCO-CN</h4> <ul><li><p><strong>数据集简介：</strong></p> <ul><li>针对图像描述任务（Image Captioning)，将英文公开数据集Flickr8K（8千图像、4万英文描述）、Flickr30K（3万图像、15万英文描述）中的英文描述使用机器翻译将翻译为中文描述，其中，测试集通过人工翻译成中文描述，构建了Flickr8K-CN、和Flickr30k-CN图像描述数据集；</li> <li>针对英文公开数据集MS-COCO的图像，增加中文描述的句子，图像约2万，人工标注的中文描述约2.7万，结合机器翻译得到的中文描述10万，构建了COCO-CN数据集</li></ul></li> <li><p><strong>数据集详情：</strong></p> <table><thead><tr><th style="text-align:center;">名称</th> <th style="text-align:center;">规模</th> <th style="text-align:center;">创建日期</th> <th style="text-align:center;">作者</th> <th style="text-align:center;">单位</th> <th style="text-align:center;">论文</th> <th style="text-align:center;">下载</th> <th style="text-align:center;">评测</th></tr></thead> <tbody><tr><td style="text-align:center;">Flickr8K-CN</td> <td style="text-align:center;">8k/45k(images/captions)</td> <td style="text-align:center;">2016年</td> <td style="text-align:center;">Xirong,Li</td> <td style="text-align:center;">人大</td> <td style="text-align:center;"><a href="https://doi.org/10.1145/2911996.2912049" target="_blank" rel="noopener noreferrer">链接<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td style="text-align:center;"><a href="https://github.com/li-xirong/cross-lingual-cap" target="_blank" rel="noopener noreferrer">链接<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td style="text-align:center;">-</td></tr> <tr><td style="text-align:center;">Flickr30K-CN</td> <td style="text-align:center;">31k/150k(images/captions)</td> <td style="text-align:center;">2017年</td> <td style="text-align:center;">Xirong,Li</td> <td style="text-align:center;">人大</td> <td style="text-align:center;"><a href="https://dl.acm.org/doi/10.1145/3123266.3123366" target="_blank" rel="noopener noreferrer">链接<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td style="text-align:center;"><a href="https://github.com/li-xirong/cross-lingual-cap" target="_blank" rel="noopener noreferrer">链接<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td style="text-align:center;">-</td></tr> <tr><td style="text-align:center;">COCO-CN</td> <td style="text-align:center;">20k/22k(images/captions)</td> <td style="text-align:center;">2019年</td> <td style="text-align:center;">Xirong,Li</td> <td style="text-align:center;">人大</td> <td style="text-align:center;"><a href="https://arxiv.org/pdf/1805.08661.pdf" target="_blank" rel="noopener noreferrer">链接<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td style="text-align:center;"><a href="https://github.com/li-xirong/cross-lingual-cap" target="_blank" rel="noopener noreferrer">链接<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td style="text-align:center;">-</td></tr></tbody></table></li> <li><p><strong>基于该数据集发表的相关论文：</strong></p> <ul><li>Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengxiong Jia, Gang Yang, Jieping Xu, COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval, IEEE Transactions on Multimedia, Volume 21, Number 9, pages 2347-2360, 2019</li> <li>Weiyu Lan, Xirong Li, Jianfeng Dong, Fluency-Guided Cross-Lingual Image Captioning, ACM MM 2017</li></ul></li></ul> <h4 id="_2-aic-icc"><a href="#_2-aic-icc" class="header-anchor">#</a> 2. AIC-ICC</h4> <ul><li><p><strong>数据集简介：</strong></p> <p>AIC-ICC(Image Chinese Captioning from AI Challenge)包含21万图像和105万图像的中文描述，涵盖日常生活常见的200多个场景，如足球场、草地等场景，唱歌、跑步等动作，是目前图像描述领域最大的中文数据集。</p></li> <li><p><strong>数据集详情：</strong></p> <table><thead><tr><th style="text-align:center;">名称</th> <th style="text-align:center;">规模</th> <th style="text-align:center;">创建日期</th> <th style="text-align:center;">作者</th> <th style="text-align:center;">单位</th> <th style="text-align:center;">论文</th> <th style="text-align:center;">下载</th> <th style="text-align:center;">评测</th></tr></thead> <tbody><tr><td style="text-align:center;">AIC-ICC</td> <td style="text-align:center;">240k/1.2M(images/captions)</td> <td style="text-align:center;">2017年</td> <td style="text-align:center;">-</td> <td style="text-align:center;">-</td> <td style="text-align:center;"><a href="https://arxiv.org/abs/1711.06475" target="_blank" rel="noopener noreferrer">链接<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td style="text-align:center;"><a href="">链接</a></td> <td style="text-align:center;">-</td></tr></tbody></table></li> <li><p><strong>基于该数据集发表的相关论文：</strong></p> <ul><li>Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengxiong Jia, Gang Yang, Jieping Xu, COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval, IEEE Transactions on Multimedia, Volume 21, Number 9, pages 2347-2360, 2019</li> <li>Song, Y., Chen, S., Zhao, Y., &amp; Jin, Q. (2019, October). Unpaired Cross-lingual Image Caption Generation with Self-Supervised Rewards. In Proceedings of the 27th ACM International Conference on Multimedia (pp. 784-792).</li></ul></li></ul> <h4 id="_3-vatex"><a href="#_3-vatex" class="header-anchor">#</a> 3. VaTEX</h4> <ul><li><p><strong>数据集简介：</strong></p> <p>VaTEX数据集对4.1万个视频进行了视频描述标注，每个视频标注了 10 个英文描述和 10 个中文描述，对比之前的MSR-VTT数据集，VaTEX更加丰富的多语言描述，基于该数据集可以进行多语言的视频描述、基于视频的翻译等任务</p></li> <li><p><strong>数据集详情：</strong></p> <table><thead><tr><th style="text-align:center;">名称</th> <th style="text-align:center;">规模</th> <th style="text-align:center;">创建日期</th> <th style="text-align:center;">作者</th> <th style="text-align:center;">单位</th> <th style="text-align:center;">论文</th> <th style="text-align:center;">下载</th> <th style="text-align:center;">评测</th></tr></thead> <tbody><tr><td style="text-align:center;">VaTEX</td> <td style="text-align:center;">41k/412k(videos/captions)</td> <td style="text-align:center;">2019年</td> <td style="text-align:center;">Xin Wang</td> <td style="text-align:center;">University of California, Santa Barbara</td> <td style="text-align:center;"><a href="https://arxiv.org/abs/1904.03493" target="_blank" rel="noopener noreferrer">链接<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td style="text-align:center;"><a href="https://vatex.org/main/index.html" target="_blank" rel="noopener noreferrer">链接<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></td> <td style="text-align:center;">-</td></tr></tbody></table></li> <li><p><strong>基于该数据集发表的相关论文：</strong></p> <ul><li>Wang, X., Wu, J., Chen, J., Li, L., Wang, Y. F., &amp; Wang, W. Y. (2019). VATEX: A large-scale, high-quality multilingual dataset for video-and-language research. In Proceedings of the IEEE International Conference on Computer Vision (pp. 4581-4591).</li></ul></li></ul> <p><a href="/github-nlp-progress/" class="router-link-active">回到首页</a></p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div> <footer class="bk-dark" data-v-2be041bb><div class="footer-content" data-v-2be041bb><p data-v-2be041bb>Published with GitHub Pages</p></div></footer></div><div class="global-ui"></div></div>
    <script src="/github-nlp-progress/assets/js/app.03014354.js" defer></script><script src="/github-nlp-progress/assets/js/2.3456af6e.js" defer></script><script src="/github-nlp-progress/assets/js/3.937c9f03.js" defer></script><script src="/github-nlp-progress/assets/js/54.9f32cbb0.js" defer></script>
  </body>
</html>
